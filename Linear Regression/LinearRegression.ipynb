{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVQ8jBUubwsN"
      },
      "source": [
        "This is the linear regression exercise. First we will implement the algorithm from scratch. Then we will implement it using a widely used library in python- Scikit Learn aka Sklearn. Note that this assignment is relatively lengthy, but nevertheless, do not be discouraged!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uqWV54-cUQP"
      },
      "source": [
        "# Data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_sfF0Bb1EEF"
      },
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQuOZNAcopv"
      },
      "source": [
        "We will be using the Boston housing dataset, a dataset containing details of houses in Boston. We will be using a subset of the actual dataset, which is easily available from sklearn. We will not be performing any exploratory data analysis as it is an extremely simple dataset with no null values and can easily be stored as numpy arrays instead of panda dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0UTtfUv1buf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f515043b-b22b-492b-9111-c0f5156da80b"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "X , y = load_boston(return_X_y = True)\n",
        "y = y.reshape(506,1)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 13) (506, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHrPG4FRdEcB"
      },
      "source": [
        "Note that m = number of training examples = 506\n",
        "and n = number of features = 13. \n",
        "\n",
        "\n",
        "\n",
        "Next we will split our dataset into a training set and testing set, a must do process in ML. We train our data using only the training test, and test our predictions on the testing set. Here we have divided the data into training and test sets in a 2:1 ratio. For more information check out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rflg4RGkd2QD"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cakhw-6q14U6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oGTcCfzhSm9"
      },
      "source": [
        "# Data Standardization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVeUNcuyd9LZ"
      },
      "source": [
        "Next we will be performing an operation called data standardization. This basically converts the data roughly into a normal distribution so that gradient descent converges to its global minimum faster. Note that you might've seen this in prob stats :D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4ii0Cmmg_pj"
      },
      "source": [
        "\"\"\" Standardization of data. We subtract the mean from each feature and then \n",
        "      divide it by the standard deviation\n",
        "\"\"\"\n",
        "mu =   np.mean(X_train,axis=0) #ADD CODE HERE to calculate mean OF EACH FEATURE OR COLUMN of the training set. Hint: use numpy\n",
        "sigma = np.std(X_train,axis=0) #ADD CODE HERE to calculate standard deviation OF EACH FEATURE of the training set\n",
        "X_train =  (X_train-mu)/sigma   #Subtract mean from training data and then divide by standard deviation\n",
        "X_test =  (X_test-mu)/sigma    #Subtract mean from testing data and then divide by standard deviation"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grNBkhV-lfkq",
        "outputId": "cd4e97e6-d26b-43b3-efd2-cee390c3dd00"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npYBRa4-hZwc"
      },
      "source": [
        "# Parameter Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWM1gHrEhewr"
      },
      "source": [
        "Now we will intialize the X matrix and theta vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlI0x_Zu2SjJ"
      },
      "source": [
        "m = y_train.shape[0]\n",
        "\n",
        "def initialize_params(X):\n",
        "  ones =  np.ones((m,1))#Numpy array of ones of size m x 1. This is the bias X0 = 1 which we add \n",
        "  X_new = np.hstack((ones,X))#Horizontally stack the bias vector to the beginning of the X matrix\n",
        "  theta = np.zeros((X_new.shape[1],1))#Numpy array of zeros. See theory to check dimensions\n",
        "  return X_new , theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5328gVhSsBZ"
      },
      "source": [
        "X_train, theta = initialize_params(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfKHz62KuLBX",
        "outputId": "7c73e863-c3ef-4bfe-8bea-1ab65de6dbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction using y=wx+b"
      ],
      "metadata": {
        "id": "AqYoyurvPm60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = y_train.shape[0]\n",
        "\n",
        "def params(X):\n",
        "  w = np.zeros((X_train.shape[1],))\n",
        "  b = 0\n",
        "  return w,b"
      ],
      "metadata": {
        "id": "1S3E7pAyzWLw"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b = params(X_train)"
      ],
      "metadata": {
        "id": "X_4wVB_I0o3r"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(w.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atGpOCdy1P0S",
        "outputId": "4a130efd-8507-4b80-f31f-88a4bab3ad27"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(339, 13)\n",
            "(13,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBFYcE32IJ_m",
        "outputId": "eade2655-e8b2-43f3-97e2-1ab7d296a356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwljJM1ti6WC"
      },
      "source": [
        "# Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function using theta"
      ],
      "metadata": {
        "id": "Tuqb7TyUP0Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(theta, X, y):\n",
        "\n",
        "  J = 1/(2*m) * np.sum( ((X@theta-y))**2 )\n",
        "  return J"
      ],
      "metadata": {
        "id": "aBnM2KuxPzfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If all code is correct you should see a cost of around 307.9\n",
        "compute_cost(theta, X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEpD67ISP8d3",
        "outputId": "52b824d0-6864-4102-a536-084bdbf2478b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "307.9009292035398"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function using w,b"
      ],
      "metadata": {
        "id": "raesDoZxQBRZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iUPcP543GI4"
      },
      "source": [
        "\n",
        "def compute_cost(X, y,w,b):\n",
        "  m,n = X.shape\n",
        "  cost = 0.0\n",
        "  for i in range(m):\n",
        "    f_x = np.dot(X[i],w)+b\n",
        "    cost = cost + (f_x-y[i])**2\n",
        "  \n",
        "  cost = cost/(2*m)\n",
        "  return (np.squeeze(cost))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBi7kb5NXYZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffd4f58-3ba2-4e78-c6fe-34e21ac2705b"
      },
      "source": [
        "compute_cost(X_train, y_train,w,b)\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(307.9009292)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oIWnQISjaIQ"
      },
      "source": [
        "# Gradient Descent \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J2TfFWtjylb"
      },
      "source": [
        "The gradient update rule for each parameter is :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KjK8X5mlE9j"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/880/1*CkcmVCUKmbA-qUn7y8srNQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqrmLimplRjH"
      },
      "source": [
        "However , if we use a for loop, computation is really slow. Therefore we used a vectorized version of the gradient update rule which is :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGugql-6ldDJ"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=17LwD2Tse6w4j4hKCZF33B0ao6yqdzeqD)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent using theta"
      ],
      "metadata": {
        "id": "7fqF28z-QP23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, theta, learning_rate, n_iters):\n",
        "    J_history = np.zeros((n_iters,1))\n",
        "\n",
        "    for i in range(n_iters):\n",
        "      J_history[i] = compute_cost(theta, X, y)\n",
        "      theta = theta - (learning_rate/m) * X.T @ (X @ theta - y) \n",
        "        \n",
        "\n",
        "    return (J_history, theta)"
      ],
      "metadata": {
        "id": "HzGGf1JfQPTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(J_history, theta) = gradient_descent(X_train, y_train, theta, 0.1, 50)"
      ],
      "metadata": {
        "id": "qeAsYW3cQUx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent using w,b"
      ],
      "metadata": {
        "id": "rG39X4IkQYvn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4yxp2m8bPXH"
      },
      "source": [
        "def compute_gradient(X, y, w,b):\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "      err = (np.dot(X[i],w)+b)-y[i]\n",
        "      for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + err * X[i,j]\n",
        "      dj_db = dj_db + err\n",
        "    dj_dw /= m\n",
        "    dj_db /= m\n",
        "\n",
        "    return dj_db,dj_dw\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy"
      ],
      "metadata": {
        "id": "MvxWQnEK6G7R"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X,y,w_in,b_in,learning_rate,num_iter):\n",
        "  m = len(X)\n",
        "  J_history = []\n",
        "  w = copy.deepcopy(w_in)\n",
        "  b = b_in\n",
        "\n",
        "  for i in range(num_iter):\n",
        "    dj_db,dj_dw = compute_gradient(X,y,w,b)\n",
        "    w = w - learning_rate * dj_dw\n",
        "    b = b - learning_rate * dj_db\n",
        "\n",
        "    J_history.append(compute_cost(X,y,w,b))\n",
        "  return J_history"
      ],
      "metadata": {
        "id": "1SPgZkt04ReA"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t513dvlanvfi"
      },
      "source": [
        "# Fitting the model and predictions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8BZPSzt4ET7"
      },
      "source": [
        "(J_history) = gradient_descent(X_train,y_train,w,b,0.1,100)#Call the gradient descent function with the training data, with a learning rate of 0.1, for 50 iterations"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZz0JhOK5zQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f8df7f-4457-4aca-8ba4-e9206239f648"
      },
      "source": [
        "print(J_history) #If all code is correct, the last value of the cost should be around 11.795"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(240.75134671), array(195.29896343), array(159.97347194), array(131.6648569), array(108.83444813), array(90.39390285), array(75.49038842), array(63.44065566), array(53.69471527), array(45.80928118), array(39.426862), array(34.25903464), array(30.07301414), array(26.680857), array(23.93077521), array(21.70014449), array(19.88987325), array(18.41986525), array(17.22536211), array(16.25399358), array(15.46339798), array(14.81930177), array(14.29396917), array(13.8649502), array(13.51406944), array(13.22660889), array(12.99064766), array(12.79652815), array(12.6364245), array(12.50399352), array(12.39409246), array(12.30255056), array(12.22598433), array(12.16164796), array(12.10731229), array(12.06116688), array(12.02174058), array(11.98783739), array(11.95848436), array(11.93288949), array(11.91040756), array(11.8905125), array(11.87277493), array(11.856844), array(11.84243268), array(11.82930579), array(11.81727033), array(11.8061676), array(11.79586683), array(11.78626002), array(11.77725773), array(11.76878565), array(11.76078185), array(11.75319453), array(11.74598017), array(11.73910202), array(11.73252897), array(11.72623446), array(11.72019573), array(11.71439318), array(11.70880978), array(11.70343068), array(11.69824282), array(11.69323465), array(11.6883959), array(11.68371736), array(11.67919073), array(11.67480849), array(11.67056376), array(11.66645026), array(11.66246219), array(11.65859418), array(11.65484125), array(11.65119874), array(11.64766228), array(11.64422779), array(11.6408914), array(11.63764946), array(11.63449853), array(11.63143533), array(11.62845676), array(11.62555984), array(11.62274176), array(11.61999982), array(11.61733144), array(11.61473417), array(11.61220565), array(11.6097436), array(11.60734588), array(11.60501039), array(11.60273514), array(11.60051821), array(11.59835776), array(11.59625201), array(11.59419926), array(11.59219787), array(11.59024625), array(11.58834287), array(11.58648628), array(11.58467505)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEo1i_nR9ax3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "29703d64-e2b0-4078-9cfe-7fb47c048cd1"
      },
      "source": [
        "plt.plot(range(len(J_history)), J_history)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Cost')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Cost')"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDle8PVUqc4a"
      },
      "source": [
        "Observe that the cost decreases rapidly at first then slows down.After the 30th iteration, there isn't much change. So training for too many iterations after 30 iterations doesn't make sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAUUCbdnjy-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f209580b-1a2f-4f5e-b26f-16025fdedf3f"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "X_test = np.hstack((np.ones((X_test.shape[0],1)) , X_test))\n",
        "y_pred = X_test @ theta\n",
        "print(0.5 * mean_squared_error(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.05982521984101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnZ-12KHsQKH"
      },
      "source": [
        "Great! Our loss on the test set is low as well. This means we have trained a pretty good model. Note that many times, the training loss will be very low while the test loss be relatively high. This is a problem known as ***overfitting*** that we will discuss later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1B6fpJnsl6m"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-AYV8pCsxHN"
      },
      "source": [
        "# Linear Regression Using Scikit Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnOj1mh4xubK"
      },
      "source": [
        "Phew! All the code above can be written in just a couple of lines using scikit learn! For this section, we leave it up to you to google the code and write it on your own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_PtOosrtKOX"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyw6ZuKWtMvM"
      },
      "source": [
        "linear_model = LinearRegression() #Create a linear regression object off the LinearRegression class\n",
        "\n",
        "#Now use the fit() method of the model on the training data below\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftSC3N_Osgho",
        "outputId": "6f23b800-ff27-4c67-aa04-ac1ade0936ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZdsyPEEtabw"
      },
      "source": [
        "#Use the predict function to get the predictions of the test and training data\n",
        "\n",
        "yPred_train = linear_model.predict(X_train)\n",
        "yPred_test = linear_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgrv7DGuv7Av",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4972637-ce10-4c8a-d50f-693daf03cf87"
      },
      "source": [
        "print(\"Training loss = \" + str(0.5 * mean_squared_error(yPred_train, y_train)))\n",
        "print(\"Test loss = \" + str(0.5 * mean_squared_error(yPred_test, y_test)))\n",
        "\n",
        "#If code is right you should see training loss of around 11.49 and test loss of 11.19"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss = 11.492507920150404\n",
            "Test loss = 10.362011718669908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fooZ-MpwGJL"
      },
      "source": [
        "Awesome! Our model from scratch performs almost as good as the one from scikit learn. Remember that we use scikit learn extensively for ML algorithms, however, we have implemented it from scratch for the sake of understanding, which is crucial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drZsxlCBwtlc"
      },
      "source": [
        "Congratulations on completing your first machine learning algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUF6PNLpww5k"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}